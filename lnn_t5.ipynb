{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4c6f12-95f1-4ea7-b9c6-f341b77b4e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.55.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ympy (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ympy (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ympy (c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77013ee-758c-49d8-9092-a8acfc1cce32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Desktop\\3rd year\\legal\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training ====\n",
      "Epoch 0 Step 0 Loss: 10.0434\n",
      "Epoch 0 Step 50 Loss: 6.7892\n",
      "Epoch 0 Step 100 Loss: 5.9301\n",
      "Epoch 0 Step 150 Loss: 4.7515\n",
      "Epoch 0 Step 200 Loss: 4.8173\n",
      "Epoch 0 Step 250 Loss: 4.3100\n",
      "Epoch 0 Step 300 Loss: 4.5681\n",
      "Epoch 0 Step 350 Loss: 4.4666\n",
      "Epoch 0 Step 400 Loss: 3.9090\n",
      "Epoch 0 Step 450 Loss: 3.3466\n",
      "Epoch 0 Step 500 Loss: 3.9505\n",
      "Epoch 0 Step 550 Loss: 4.4285\n",
      "Epoch 0 Step 600 Loss: 4.3987\n",
      "Epoch 0 Step 650 Loss: 4.2467\n",
      "Epoch 0 Step 700 Loss: 4.2485\n",
      "Epoch 0 Step 750 Loss: 3.4759\n",
      "Epoch 0 Step 800 Loss: 3.8752\n",
      "Epoch 0 Step 850 Loss: 4.0699\n",
      "Epoch 0 Step 900 Loss: 3.6458\n",
      "Epoch 0 Step 950 Loss: 4.1561\n",
      "Epoch 0 Step 1000 Loss: 3.9061\n",
      "Epoch 0 Step 1050 Loss: 4.1338\n",
      "Epoch 0 Step 1100 Loss: 3.3210\n",
      "Epoch 0 Step 1150 Loss: 4.0493\n",
      "Epoch 0 Step 1200 Loss: 3.4245\n",
      "Epoch 0 Step 1250 Loss: 4.2910\n",
      "Epoch 0 Step 1300 Loss: 3.7003\n",
      "Epoch 0 Step 1350 Loss: 4.3048\n",
      "Epoch 0 Step 1400 Loss: 3.9976\n",
      "Epoch 0 Step 1450 Loss: 4.1789\n",
      "Epoch 0 Step 1500 Loss: 3.9657\n",
      "Epoch 0 Step 1550 Loss: 3.2164\n",
      "Epoch 0 Step 1600 Loss: 3.9688\n",
      "Epoch 0 Step 1650 Loss: 3.7305\n",
      "Epoch 0 Step 1700 Loss: 3.1277\n",
      "Epoch 0 Step 1750 Loss: 3.9555\n",
      "Epoch 1 Step 0 Loss: 3.4376\n",
      "Epoch 1 Step 50 Loss: 3.3051\n",
      "Epoch 1 Step 100 Loss: 4.0974\n",
      "Epoch 1 Step 150 Loss: 3.7140\n",
      "Epoch 1 Step 200 Loss: 3.7471\n",
      "Epoch 1 Step 250 Loss: 3.7547\n",
      "Epoch 1 Step 300 Loss: 3.9074\n",
      "Epoch 1 Step 350 Loss: 3.8754\n",
      "Epoch 1 Step 400 Loss: 4.0554\n",
      "Epoch 1 Step 450 Loss: 3.4504\n",
      "Epoch 1 Step 500 Loss: 3.8549\n",
      "Epoch 1 Step 550 Loss: 3.6313\n",
      "Epoch 1 Step 600 Loss: 3.5017\n",
      "Epoch 1 Step 650 Loss: 3.6894\n",
      "Epoch 1 Step 700 Loss: 3.6369\n",
      "Epoch 1 Step 750 Loss: 3.9252\n",
      "Epoch 1 Step 800 Loss: 2.9753\n",
      "Epoch 1 Step 850 Loss: 4.1834\n",
      "Epoch 1 Step 900 Loss: 3.8955\n",
      "Epoch 1 Step 950 Loss: 3.8565\n",
      "Epoch 1 Step 1000 Loss: 3.8592\n",
      "Epoch 1 Step 1050 Loss: 3.3612\n",
      "Epoch 1 Step 1100 Loss: 3.8370\n",
      "Epoch 1 Step 1150 Loss: 3.6401\n",
      "Epoch 1 Step 1200 Loss: 3.7338\n",
      "Epoch 1 Step 1250 Loss: 2.9096\n",
      "Epoch 1 Step 1300 Loss: 3.9909\n",
      "Epoch 1 Step 1350 Loss: 3.8904\n",
      "Epoch 1 Step 1400 Loss: 3.5787\n",
      "Epoch 1 Step 1450 Loss: 3.0358\n",
      "Epoch 1 Step 1500 Loss: 3.4188\n",
      "Epoch 1 Step 1550 Loss: 3.2481\n",
      "Epoch 1 Step 1600 Loss: 3.7083\n",
      "Epoch 1 Step 1650 Loss: 3.9857\n",
      "Epoch 1 Step 1700 Loss: 3.4594\n",
      "Epoch 1 Step 1750 Loss: 2.7739\n",
      "Epoch 2 Step 0 Loss: 3.9188\n",
      "Epoch 2 Step 50 Loss: 3.5343\n",
      "Epoch 2 Step 100 Loss: 3.7970\n",
      "Epoch 2 Step 150 Loss: 4.1352\n",
      "Epoch 2 Step 200 Loss: 3.9160\n",
      "Epoch 2 Step 250 Loss: 3.8484\n",
      "Epoch 2 Step 300 Loss: 3.5903\n",
      "Epoch 2 Step 350 Loss: 3.8545\n",
      "Epoch 2 Step 400 Loss: 3.7860\n",
      "Epoch 2 Step 450 Loss: 3.7446\n",
      "Epoch 2 Step 500 Loss: 3.8619\n",
      "Epoch 2 Step 550 Loss: 3.4310\n",
      "Epoch 2 Step 600 Loss: 3.0887\n",
      "Epoch 2 Step 650 Loss: 3.0495\n",
      "Epoch 2 Step 700 Loss: 3.4410\n",
      "Epoch 2 Step 750 Loss: 2.6223\n",
      "Epoch 2 Step 800 Loss: 4.0831\n",
      "Epoch 2 Step 850 Loss: 3.7903\n",
      "Epoch 2 Step 900 Loss: 3.7398\n",
      "Epoch 2 Step 950 Loss: 3.5086\n",
      "Epoch 2 Step 1000 Loss: 3.8847\n",
      "Epoch 2 Step 1050 Loss: 3.6715\n",
      "Epoch 2 Step 1100 Loss: 3.7722\n",
      "Epoch 2 Step 1150 Loss: 4.1053\n",
      "Epoch 2 Step 1200 Loss: 3.4894\n",
      "Epoch 2 Step 1250 Loss: 3.2191\n",
      "Epoch 2 Step 1300 Loss: 3.1387\n",
      "Epoch 2 Step 1350 Loss: 3.3413\n",
      "Epoch 2 Step 1400 Loss: 3.5942\n",
      "Epoch 2 Step 1450 Loss: 3.5688\n",
      "Epoch 2 Step 1500 Loss: 3.9761\n",
      "Epoch 2 Step 1550 Loss: 3.5279\n",
      "Epoch 2 Step 1600 Loss: 3.8881\n",
      "Epoch 2 Step 1650 Loss: 3.0019\n",
      "Epoch 2 Step 1700 Loss: 3.6552\n",
      "Epoch 2 Step 1750 Loss: 3.7509\n",
      "==== Evaluating with ROUGE ====\n",
      "ROUGE Scores: {'rouge1': np.float64(0.18985063152273518), 'rouge2': np.float64(0.04420249942202605), 'rougeL': np.float64(0.15552658100605676), 'rougeLsum': np.float64(0.1556816021701396)}\n"
     ]
    }
   ],
   "source": [
    "# pip install torch transformers datasets evaluate\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import evaluate\n",
    "\n",
    "# =====================\n",
    "# Liquid Neural Network\n",
    "# =====================\n",
    "class LiquidTimeStep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W_in = nn.Linear(input_size, hidden_size)\n",
    "        self.W_h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tau = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        dx = torch.tanh(self.W_in(x) + self.W_h(h))\n",
    "        h_new = h + (dx - h) / self.tau\n",
    "        return h_new\n",
    "\n",
    "class LiquidNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.liquid_step = LiquidTimeStep(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        for t in range(seq_len):\n",
    "            h = self.liquid_step(x[:, t, :], h)\n",
    "        return self.output_layer(h)\n",
    "\n",
    "# =====================\n",
    "# Dataset Loader\n",
    "# =====================\n",
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, tokenizer, max_length=512):\n",
    "        self.data = []\n",
    "        judg_dir = os.path.join(root_dir, split, 'judgement')\n",
    "        summ_dir = os.path.join(root_dir, split, 'summary')\n",
    "\n",
    "        for fname in os.listdir(judg_dir):\n",
    "            if fname.endswith('.txt') and os.path.exists(os.path.join(summ_dir, fname)):\n",
    "                with open(os.path.join(judg_dir, fname), 'r', encoding='utf-8') as f1, \\\n",
    "                     open(os.path.join(summ_dir, fname), 'r', encoding='utf-8') as f2:\n",
    "                    judgement = f1.read().strip()\n",
    "                    summary = f2.read().strip()\n",
    "                self.data.append((judgement, summary))\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        judgement, summary = self.data[idx]\n",
    "        enc_j = self.tokenizer(judgement, return_tensors='pt',\n",
    "                               truncation=True, padding='max_length',\n",
    "                               max_length=self.max_length)\n",
    "        enc_s = self.tokenizer(summary, return_tensors='pt',\n",
    "                               truncation=True, padding='max_length',\n",
    "                               max_length=self.max_length)\n",
    "        return enc_j.input_ids.squeeze(0), enc_s.input_ids.squeeze(0)\n",
    "\n",
    "# =====================\n",
    "# Training & Evaluation\n",
    "# =====================\n",
    "def train_and_evaluate(root_dir, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    t5 = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "    liquid = LiquidNeuralNetwork(\n",
    "        input_size=1,  # token IDs as single feature\n",
    "        hidden_size=256,\n",
    "        output_size=t5.config.d_model\n",
    "    ).to(device)\n",
    "\n",
    "    train_data = LegalDataset(root_dir, 'train-data', tokenizer)\n",
    "    test_data = LegalDataset(root_dir, 'test-data', tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=4)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(liquid.parameters()) + list(t5.parameters()), lr=5e-5)\n",
    "\n",
    "    print(\"==== Training ====\")\n",
    "    for epoch in range(3):\n",
    "        t5.train()\n",
    "        for idx, (ids_judg, ids_summ) in enumerate(train_loader):\n",
    "            ids_judg = ids_judg.to(device).float().unsqueeze(-1)  # [batch, seq, 1]\n",
    "            ids_summ = ids_summ.to(device)\n",
    "\n",
    "            emb = liquid(ids_judg).unsqueeze(1)  # [batch, 1, hidden_size]\n",
    "            outputs = t5(input_ids=None,\n",
    "                         encoder_outputs=BaseModelOutput(last_hidden_state=emb),\n",
    "                         labels=ids_summ)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch} Step {idx} Loss: {loss.item():.4f}\")\n",
    "\n",
    "    torch.save(liquid.state_dict(), 'liquid_model.pth')\n",
    "    t5.save_pretrained('t5_liquid_model')\n",
    "    tokenizer.save_pretrained('t5_liquid_model')\n",
    "\n",
    "    print(\"==== Evaluating with ROUGE ====\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    t5.eval()\n",
    "\n",
    "    predictions, references = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids_judg, ids_summ in test_loader:\n",
    "            ids_judg = ids_judg.to(device).float().unsqueeze(-1)\n",
    "            ids_summ = ids_summ.to(device)\n",
    "\n",
    "            emb = liquid(ids_judg).unsqueeze(1)\n",
    "            encoder_outputs = BaseModelOutput(last_hidden_state=emb)\n",
    "            generated_ids = t5.generate(encoder_outputs=encoder_outputs, max_length=128)\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            decoded_refs = tokenizer.batch_decode(ids_summ, skip_special_tokens=True)\n",
    "\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_refs)\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# =====================\n",
    "# Run\n",
    "# =====================\n",
    "if __name__ == '__main__':\n",
    "    # Point to the INNER folder\n",
    "    data_root = 'IN-Abs/IN-Abs'\n",
    "    train_and_evaluate(data_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465c9fa-b585-4245-8a81-95170ba98952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
